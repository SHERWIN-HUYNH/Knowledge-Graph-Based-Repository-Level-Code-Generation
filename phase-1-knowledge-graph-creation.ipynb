{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13287788,"sourceType":"datasetVersion","datasetId":8421388}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phase 1: Knowledge Graph Construction","metadata":{}},{"cell_type":"code","source":"# --- C√†i ƒë·∫∑t ---\n# --- C√ÄI ƒê·∫∂T M√îI TR∆Ø·ªúNG ---\n# C·∫≠p nh·∫≠t h·ªá th·ªëng v√† c√†i c√°c c√¥ng c·ª• c·∫ßn thi·∫øt (ch·ªâ ch·∫°y l·∫ßn ƒë·∫ßu)\n!apt-get update -qq && apt-get install -y build-essential git > /dev/null 2>&1\n\n# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán Python c·∫ßn thi·∫øt\n!pip install -q -U \\\n    neo4j \\\n    tree-sitter \\\n    sentence-transformers \\\n    langchain \\\n    langchain-openai \\\n    langchain-community \\\n    tqdm \\\n    python-dotenv\n\n# C√†i ƒë·∫∑t c√°c g√≥i ng·ªØ ph√°p Tree-sitter ƒë√£ bi√™n d·ªãch s·∫µn\n!pip install -q \\\n    tree-sitter-python \\\n    tree-sitter-java \\\n    tree-sitter-javascript \\\n    tree-sitter-typescript \\\n    tree-sitter-cpp \\\n    tree-sitter-c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:56:04.404879Z","iopub.execute_input":"2025-10-30T06:56:04.408054Z","iopub.status.idle":"2025-10-30T06:56:22.731874Z","shell.execute_reply.started":"2025-10-30T06:56:04.407976Z","shell.execute_reply":"2025-10-30T06:56:22.730110Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# --- Imports ---\nimport os\nimport glob\nimport json\nimport fnmatch\nimport requests \nimport re\nfrom tqdm.auto import tqdm\nfrom typing import List, Dict, Tuple, Set, Optional\nfrom langchain_openai import ChatOpenAI\nfrom neo4j import GraphDatabase\nfrom langchain_core.messages import HumanMessage\n\nfrom sentence_transformers import SentenceTransformer\nfrom tree_sitter import Language, Parser, Query, QueryCursor\nfrom kaggle_secrets import UserSecretsClient \n\n# --- Configuration (V√≠ d·ª•) ---\nclass Config:\n    REPO_ROOT_DIR = '/kaggle/input/chatbot/chatbotai_v1-main'\n    NEO4J_URI_SECRET = \"NEO4J_URI\"\n    NEO4J_USER_SECRET = \"NEO4J_USER\"\n    NEO4J_PASSWORD_SECRET = \"NEO4J_PASSWORD\"\n    OPENAI_API_KEY_SECRET = \"OPENAI_API_KEY\"\n    OPENAI_MODEL = \"gpt-4o\"\n    # OLLAMA_BASE_URL = \"http://192.168.92.23:11434\"\n    # OLLAMA_MODEL = \"qwen3-embedding:8b\" \n    ENCODER_MODEL = 'all-MiniLM-L6-v2'\n    BATCH_SIZE = 500 # K√≠ch th∆∞·ªõc l√¥ cho Neo4j ingest\n    MAX_CODE_LENGTH = 2048 # Gi·ªõi h·∫°n ƒë·ªô d√†i code l∆∞u tr·ªØ\n    ENABLE_PROGRESS_BAR = True # B·∫≠t/t·∫Øt thanh ti·∫øn tr√¨nh\n    SKIP_FOLDERS = ['.*', '*venv*', 'node_modules', '__pycache__', 'build', 'dist', 'docs', 'tests', 'examples'] # Th∆∞ m·ª•c b·ªè qua\n    SKIP_FILES = ['*setup.py', '*config.js', '*.min.js', '*.css', '*.html', '*.md', '*.txt', '*.json', '*.yaml', '*.yml', 'LICENSE', '.*'] # File b·ªè qua","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-10-30T09:38:59.107103Z","iopub.execute_input":"2025-10-30T09:38:59.108225Z","iopub.status.idle":"2025-10-30T09:38:59.119418Z","shell.execute_reply.started":"2025-10-30T09:38:59.108191Z","shell.execute_reply":"2025-10-30T09:38:59.117952Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"try:\n    from tree_sitter_python import language as python_lang_loader\n    from tree_sitter_java import language as java_lang_loader\n    from tree_sitter_javascript import language as js_lang_loader\n    # import tree_sitter_typescript.typescript as ts\n    # import tree_sitter_typescript.tsx as tsx\n\n    print(\"ƒêang t·∫£i c√°c ƒë·ªëi t∆∞·ª£ng Ng√¥n ng·ªØ...\")\n\n    PY_LANG = Language(python_lang_loader())\n    JAVA_LANG = Language(java_lang_loader())\n    JS_LANG = Language(js_lang_loader()) # D√πng cho c·∫£ .js v√† .jsx, .ts, .tsx\n    # TS_LANG = Language(ts.language())\n    # TSX_LANG = Language(tsx.language())\n\n    # D√πng js cho c·∫£ ts v√† tsx\n    # *** ƒê√ÇY L√Ä PH·∫¶N S·ª¨A L·ªñI QUAN TR·ªåNG ***\n    LANGUAGE_MAP = {\n    '.py': (PY_LANG, 'python'),\n    '.java': (JAVA_LANG, 'java'),\n    '.js': (JS_LANG, 'javascript'),\n    '.jsx': (JS_LANG, 'javascript')\n    # '.ts': (TS_LANG, 'typescript'),\n    # '.tsx': (TSX_LANG, 'typescriptreact')\n}\n\n    print(f\"‚úì ƒê√£ t·∫£i {len(LANGUAGE_MAP)} ng√¥n ng·ªØ/ƒëu√¥i file.\")\n\nexcept Exception as e:\n    print(f\"‚ö† L·ªói t·∫£i ng√¥n ng·ªØ: {e}\")\n    raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:03:52.929707Z","iopub.execute_input":"2025-10-30T07:03:52.930116Z","iopub.status.idle":"2025-10-30T07:03:52.938524Z","shell.execute_reply.started":"2025-10-30T07:03:52.930093Z","shell.execute_reply":"2025-10-30T07:03:52.937590Z"}},"outputs":[{"name":"stdout","text":"ƒêang t·∫£i c√°c ƒë·ªëi t∆∞·ª£ng Ng√¥n ng·ªØ...\n‚úì ƒê√£ t·∫£i 4 ng√¥n ng·ªØ/ƒëu√¥i file.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# --- Ph·∫ßn 2: Tr√¨nh k·∫øt n·ªëi Neo4j (Database Connector) ---\n\nclass Neo4jGraphConstructor:\n    \"\"\"Constructor cho Neo4j Knowledge Graph\"\"\"\n\n    def __init__(self, uri: str, user: str, password: str):\n        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n        print(\"‚úì Connected to Neo4j AuraDB\")\n\n    def close(self):\n        self.driver.close()\n        print(\"‚úì Disconnected from Neo4j\")\n\n    def run_cypher_query(self, query: str, params: Optional[Dict] = None) -> Optional[List]:\n        with self.driver.session() as session:\n            try:\n                result = session.run(query, params)\n                return [record for record in result]\n            except Exception as e:\n                print(f\"‚ö† Cypher error: {e}\")\n                return None\n\n    def clear_database(self):\n        \"\"\"X√≥a to√†n b·ªô database (c·∫©n th·∫≠n!)\"\"\"\n        print(\"\\nüóëÔ∏è  Clearing database...\")\n        self.run_cypher_query(\"MATCH (n) DETACH DELETE n\")\n        print(\"‚úì Database cleared\")\n\n    def create_constraints_and_indexes(self):\n        \"\"\"T·∫°o constraints v√† indexes theo schema c·∫£i thi·ªán\"\"\"\n        print(\"\\nüîß Creating constraints and indexes...\")\n\n        # Constraints (Bao g·ªìm Placeholder, Definition, Attribute, Documentation)\n        constraints = [\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (f:File) REQUIRE f.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Class) REQUIRE c.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (fn:Function) REQUIRE fn.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Method) REQUIRE m.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Placeholder) REQUIRE p.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (d:GeneratedDescription) REQUIRE d.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (cd:ClassDefinition) REQUIRE cd.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (fd:FunctionDefinition) REQUIRE fd.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (md:MethodDefinition) REQUIRE md.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (a:Attribute) REQUIRE a.fqn IS UNIQUE\",\n            \"CREATE CONSTRAINT IF NOT EXISTS FOR (doc:Documentation) REQUIRE doc.fqn IS UNIQUE\" # Th√™m cho Documentation\n        ]\n        for constraint in constraints:\n            self.run_cypher_query(constraint)\n\n        # Full-text index (Bao g·ªìm Placeholder)\n        self.run_cypher_query(\"\"\"\n            CREATE FULLTEXT INDEX names IF NOT EXISTS\n            FOR (n:Class|Function|Method|Attribute|Placeholder)\n            ON EACH [n.name]\n            OPTIONS { indexConfig: { `fulltext.analyzer`: 'standard' } }\n        \"\"\")\n\n        # Vector index 1 (cho GeneratedDescription)\n        self.run_cypher_query(\"\"\"\n            CREATE VECTOR INDEX generated_descriptions IF NOT EXISTS\n            FOR (n:GeneratedDescription)\n            ON (n.embedding)\n            OPTIONS { indexConfig: {\n                `vector.dimensions`: 384,\n                `vector.similarity_function`: 'cosine'\n            }}\n        \"\"\")\n\n        # Vector index 2 (cho Documentation)\n        self.run_cypher_query(\"\"\"\n            CREATE VECTOR INDEX documentation_embeddings IF NOT EXISTS\n            FOR (n:Documentation)\n            ON (n.embedding)\n            OPTIONS { indexConfig: {\n                `vector.dimensions`: 384,\n                `vector.similarity_function`: 'cosine'\n            }}\n        \"\"\")\n\n        print(\"‚úì Constraints and indexes created\")\n\n    def ingest_data(self, nodes: List[Dict], relationships: List[Dict]):\n        \"\"\"Ingest nodes v√† relationships v√†o Neo4j theo l√¥\"\"\"\n        print(f\"\\nüì• Ingesting data to Neo4j...\")\n        print(f\"  - Nodes to ingest: {len(nodes)}\")\n        print(f\"  - Relationships to ingest: {len(relationships)}\")\n\n        # Batch insert nodes\n        self._batch_insert_nodes(nodes)\n        # Batch insert relationships\n        self._batch_insert_relationships(relationships)\n\n        print(\"‚úì Data ingestion complete\")\n\n    def _batch_insert_nodes(self, nodes: List[Dict]):\n        \"\"\"Batch insert nodes\"\"\"\n        query = \"\"\"\n        UNWIND $nodes AS node_data\n        MERGE (n {fqn: node_data.fqn})\n        ON CREATE SET n += node_data, n:Node\n        ON MATCH SET n += node_data, n:Node\n        WITH n, node_data\n        // Ch·ªâ g·ªçi addLabels n·∫øu node_data.type t·ªìn t·∫°i v√† h·ª£p l·ªá\n        WHERE node_data.type IS NOT NULL AND node_data.type <> ''\n        CALL apoc.create.addLabels(n, [node_data.type]) YIELD node\n        RETURN count(node)\n        \"\"\"\n        total_inserted = 0\n        iterable = range(0, len(nodes), Config.BATCH_SIZE)\n        if Config.ENABLE_PROGRESS_BAR:\n             iterable = tqdm(iterable, desc=\"  Ingesting Nodes\", unit=\"batch\")\n\n        for i in iterable:\n            batch = nodes[i:i + Config.BATCH_SIZE]\n            result = self.run_cypher_query(query, params={'nodes': batch})\n            if result:\n                 total_inserted += result[0]['count(node)'] if result[0]['count(node)'] else 0\n                 # C·∫≠p nh·∫≠t progress bar n·∫øu d√πng tqdm\n                 # if isinstance(iterable, tqdm): iterable.set_postfix({\"inserted\": total_inserted})\n\n\n        print(f\"  ‚úì Processed {len(nodes)} nodes (actual inserts depend on MERGE)\")\n\n\n    def _batch_insert_relationships(self, relationships: List[Dict]):\n        \"\"\"Batch insert relationships\"\"\"\n        if not relationships:\n            print(\"  ‚úì No relationships to insert.\")\n            return\n\n        query = \"\"\"\n        UNWIND $relationships AS rel_data\n        MATCH (source {fqn: rel_data.source_fqn})\n        MATCH (target {fqn: rel_data.target_fqn})\n        // S·ª≠ d·ª•ng MERGE thay v√¨ CALL apoc.create.relationship ƒë·ªÉ tr√°nh tr√πng l·∫∑p\n        // C·∫ßn ƒë·∫£m b·∫£o rel_data.type l√† t√™n h·ª£p l·ªá\n        CALL apoc.merge.relationship(source, rel_data.type, {}, rel_data.properties, target) YIELD rel\n        RETURN count(rel)\n        \"\"\"\n        # L∆∞u √Ω: CALL apoc.merge.relationship y√™u c·∫ßu APOC.\n        # N·∫øu kh√¥ng c√≥ APOC, b·∫°n c·∫ßn t·∫°o query ri√™ng cho t·ª´ng lo·∫°i relationship:\n        # MERGE (source)-[r:<REL_TYPE>]->(target) SET r += rel_data.properties\n\n        total_inserted = 0\n        iterable = range(0, len(relationships), Config.BATCH_SIZE)\n        if Config.ENABLE_PROGRESS_BAR:\n            iterable = tqdm(iterable, desc=\"  Ingesting Relationships\", unit=\"batch\")\n\n        for i in iterable:\n            batch = relationships[i:i + Config.BATCH_SIZE]\n            result = self.run_cypher_query(query, params={'relationships': batch})\n            if result:\n                total_inserted += result[0]['count(rel)'] if result[0]['count(rel)'] else 0\n                # if isinstance(iterable, tqdm): iterable.set_postfix({\"inserted\": total_inserted})\n\n\n        print(f\"  ‚úì Processed {len(relationships)} relationships (actual inserts depend on MERGE)\")\n\n\n    def verify_graph(self):\n        \"\"\"Verify graph statistics\"\"\"\n        print(\"\\nüìä Graph Statistics:\")\n        # Node counts by type (l·∫•y label ƒë·∫ßu ti√™n ngo√†i :Node)\n        node_query = \"\"\"\n        MATCH (n)\n        WHERE size(labels(n)) > 1\n        RETURN labels(n)[1] as type, count(n) as count\n        ORDER BY count DESC\n        \"\"\"\n        results = self.run_cypher_query(node_query)\n        if results:\n            print(\"\\n  Node Types (excluding :Node):\")\n            for record in results:\n                print(f\"    {record['type']}: {record['count']}\")\n        # Relationship counts by type (Gi·ªØ nguy√™n)\n        rel_query = \"MATCH ()-[r]->() RETURN type(r) as type, count(r) as count ORDER BY count DESC\"\n        results = self.run_cypher_query(rel_query)\n        if results:\n            print(\"\\n  Relationship Types:\")\n            for record in results:\n                print(f\"    {record['type']}: {record['count']}\")\n        # Total counts (Gi·ªØ nguy√™n)\n        total_query = \"MATCH (n) WITH count(n) as node_count MATCH ()-[r]->() RETURN node_count, count(r) as rel_count\"\n        results = self.run_cypher_query(total_query)\n        if results:\n            record = results[0]\n            print(f\"\\n  Total Nodes: {record['node_count']}\")\n            print(f\"  Total Relationships: {record['rel_count']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:03:58.547180Z","iopub.execute_input":"2025-10-30T07:03:58.547625Z","iopub.status.idle":"2025-10-30T07:03:58.573469Z","shell.execute_reply.started":"2025-10-30T07:03:58.547589Z","shell.execute_reply":"2025-10-30T07:03:58.572371Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# --- Ph·∫ßn 4: Repository Parser ---\n\nclass RepositoryParser:\n    \"\"\"Parse to√†n b·ªô repository, l·ªçc b·ªè file/folder kh√¥ng c·∫ßn thi·∫øt\"\"\"\n\n    def __init__(self, repo_path: str, language_map: Dict):\n        self.repo_path = os.path.abspath(repo_path) # Chu·∫©n h√≥a ƒë∆∞·ªùng d·∫´n\n        self.language_map = language_map\n        self.all_nodes: List[Dict] = []\n        self.all_relationships: List[Dict] = []\n\n    def parse_repository(self) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"Parse t·∫•t c·∫£ files trong repo\"\"\"\n        print(f\"\\nüìÇ Parsing repository: {self.repo_path}\")\n\n        files_to_parse = self._collect_files_with_os_walk() # S·ª≠ d·ª•ng os.walk\n        print(f\"Found {len(files_to_parse)} files to parse after filtering\")\n\n        iterable = files_to_parse\n        if Config.ENABLE_PROGRESS_BAR:\n            iterable = tqdm(files_to_parse, desc=\"Parsing files\", unit=\"file\")\n\n        for file_path in iterable:\n            self._parse_file(file_path)\n\n        print(f\"\\n‚úì Parsing complete!\")\n        print(f\"  - Total nodes extracted: {len(self.all_nodes)}\")\n        print(f\"  - Total relationships extracted: {len(self.all_relationships)}\")\n\n        return self.all_nodes, self.all_relationships\n\n    def _collect_files_with_os_walk(self) -> List[str]:\n        \"\"\"Thu th·∫≠p files d√πng os.walk v√† l·ªçc b·ªè\"\"\"\n        collected_files = []\n        exclude_dir_patterns = Config.SKIP_FOLDERS\n        exclude_file_patterns = Config.SKIP_FILES\n\n        for root, dirs, files in os.walk(self.repo_path, topdown=True):\n            # L·ªçc b·ªè th∆∞ m·ª•c kh√¥ng mong mu·ªën\n            # ':' modifies dirs in-place\n            dirs[:] = [d for d in dirs if not any(fnmatch.fnmatch(d, pattern) for pattern in exclude_dir_patterns)]\n\n            for filename in files:\n                # L·ªçc b·ªè file kh√¥ng mong mu·ªën\n                if any(fnmatch.fnmatch(filename, pattern) for pattern in exclude_file_patterns):\n                    continue\n\n                # Ch·ªâ l·∫•y file c√≥ ƒëu√¥i h·ªó tr·ª£\n                file_ext = os.path.splitext(filename)[1]\n                if file_ext in self.language_map:\n                    collected_files.append(os.path.join(root, filename))\n\n        return sorted(collected_files)\n\n\n    def _parse_file(self, file_path: str):\n        \"\"\"Parse m·ªôt file - S·ª¨A L·ªñI: ƒê·ªçc file d∆∞·ªõi d·∫°ng bytes ('rb')\"\"\"\n        ext = os.path.splitext(file_path)[1]\n        lang_tuple = self.language_map.get(ext)\n        if not lang_tuple: return\n\n        try:\n            # S·ª¨A L·ªñI: M·ªü file ·ªü ch·∫ø ƒë·ªô read-bytes ('rb')\n            # v√† kh√¥ng ch·ªâ ƒë·ªãnh encoding\n            with open(file_path, 'rb') as f:\n                code_content_bytes = f.read()\n\n            # Gi·ªõi h·∫°n k√≠ch th∆∞·ªõc file\n            if len(code_content_bytes) > 500 * 1024: # Gi·ªõi h·∫°n 500KB\n                 print(f\"  üü° Skipping large file: {file_path} ({len(code_content_bytes)/1024:.1f} KB)\")\n                 return\n\n            if not code_content_bytes.strip(): # B·ªè qua file tr·ªëng\n                 print(f\"  üü° Skipping empty file: {file_path}\")\n                 return\n\n            # S·ª¨A L·ªñI: Truy·ªÅn 'code_content_bytes' (l√† bytes)\n            # v√†o constructor c·ªßa parser\n            parser = EnhancedTreeSitterParser(file_path, code_content_bytes, lang_tuple)\n            nodes, rels = parser.parse()\n\n            self.all_nodes.extend(nodes)\n            self.all_relationships.extend(rels)\n\n        except UnicodeDecodeError:\n             # L·ªói n√†y b√¢y gi·ªù kh√¥ng n√™n x·∫£y ra ·ªü ƒë√¢y,\n             # nh∆∞ng c√≥ th·ªÉ x·∫£y ra trong get_node_text\n             print(f\"  ‚ö† UnicodeDecodeError parsing {file_path}. Skipping.\")\n        except Exception as e:\n            print(f\"  ‚ö† Error parsing {file_path}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:17:48.030932Z","iopub.execute_input":"2025-10-30T09:17:48.031317Z","iopub.status.idle":"2025-10-30T09:17:48.051512Z","shell.execute_reply.started":"2025-10-30T09:17:48.031292Z","shell.execute_reply":"2025-10-30T09:17:48.049891Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport os, fnmatch, re\nfrom typing import Dict, List, Tuple, Optional, Set\nfrom tree_sitter import Parser, Query, QueryCursor\n\n# Assumed available in your environment\n# from your_config import Config\n\nclass EnhancedTreeSitterParser:\n    \"\"\"\n    Parser theo paper, ƒë√£ FIX:\n    - ƒê·ªçc n·ªôi dung file d·∫°ng bytes ƒë·ªÉ tr√°nh l·ªói encoding.\n    - Docstring: query n·ªõi l·ªèng + x√°c th·ª±c ƒë√∫ng v·ªã tr√≠, l√†m s·∫°ch theo ng√¥n ng·ªØ.\n    - CALLS: truy v·∫•n ƒë√∫ng node theo grammar t·ª´ng ng√¥n ng·ªØ (identifier/attribute/member_expression),\n      chu·∫©n h√≥a t√™n h√†m ƒë∆∞·ª£c g·ªçi ƒë·ªÉ tr√°nh c·∫Øt chu·ªói sai (v√≠ d·ª• 'lude_router(ch').\n    \"\"\"\n\n    def __init__(self, file_path: str, code_content_bytes: bytes, lang_tuple: Tuple):\n        self.file_path = file_path\n        self.code_content_bytes = code_content_bytes\n        self.language_obj = lang_tuple[0]\n        self.language_name = lang_tuple[1]\n\n        self.parser = Parser()\n        self.parser.language = self.language_obj\n\n        self.nodes: List[Dict] = []\n        self.relationships: List[Dict] = []\n        self._processed_fqns: Set[str] = set()\n        self.queries: Dict = self._init_queries()\n\n    def _init_queries(self) -> Dict:\n        \"\"\"\n        Kh·ªüi t·∫°o query h·ª£p l·ªá cho t·ª´ng ng√¥n ng·ªØ.\n        L∆∞u √Ω: KH√îNG d√πng 'function: @call_name' tr·ªëng ‚Äî ph·∫£i n√™u r√µ node type.\n        \"\"\"\n        return {\n            'python': {\n                'class': \"(class_definition name: (identifier) @class_name)\",\n                'function': \"(function_definition name: (identifier) @function_name)\",\n                'method': \"(class_definition body: (block (function_definition name: (identifier) @method_name) @method_node) @class_node)\",\n                'attribute': \"(class_definition body: (block (expression_statement (assignment left: (identifier) @attr_name)))) @class_node\",\n\n                'import_from': \"(import_from_statement module_name: (dotted_name) @module)\",\n                'import_simple': \"(import_statement name: (dotted_name) @module)\",\n\n                # CALLS: 2 d·∫°ng ‚Äî g·ªçi tr·ª±c ti·∫øp v√† qua thu·ªôc t√≠nh\n                'call_py_identifier': \"(call function: (identifier) @call_name)\",\n                'call_py_attribute': \"(call function: (attribute attribute: (identifier) @call_attr_name))\",\n\n                # Docstring: string ƒë·∫ßu block\n                'docstring': \"\"\"\n                    (function_definition body: (block (expression_statement (string) @docstring))) @func\n                    (class_definition body: (block (expression_statement (string) @docstring))) @class\n                \"\"\"\n            },\n\n            'java': {\n                'class': \"(class_declaration name: (identifier) @class_name)\",\n                'function': \"(method_declaration name: (identifier) @function_name)\",\n                'method': \"(class_declaration body: (class_body (method_declaration name: (identifier) @method_name) @method_node) @class_node)\",\n                'attribute': \"(class_declaration body: (class_body (field_declaration declarator: (variable_declarator name: (identifier) @attr_name)))) @class_node\",\n\n                'import_from': \"(import_declaration (scoped_identifier) @module)\",\n\n                # CALLS: Java d√πng method_invocation\n                'call_java_name': \"(method_invocation name: (identifier) @call_name)\",\n                'call_java_scoped': \"(method_invocation name: (scoped_identifier) @call_scoped)\",\n\n                # Docstring/JavaDoc: comment ngay tr∆∞·ªõc khai b√°o\n                'docstring': \"\"\"\n                    (method_declaration (comment) @docstring) @func\n                    (class_declaration (comment) @docstring) @class\n                \"\"\"\n            },\n\n            'javascript': {\n                'class': \"(class_declaration name: (identifier) @class_name)\",\n                'function': \"(function_declaration name: (identifier) @function_name)\",\n                'method': \"(class_declaration body: (class_body (method_definition name: (property_identifier) @method_name) @method_node) @class_node)\",\n                'attribute': \"(class_declaration body: (class_body (public_field_definition name: (property_identifier) @attr_name))) @class_node\",\n\n                'import_from': \"(import_statement source: (string) @module)\",\n\n                # CALLS: 2 d·∫°ng ‚Äî g·ªçi tr·ª±c ti·∫øp v√† qua member_expression\n                'call_js_identifier': \"(call_expression function: (identifier) @call_name)\",\n                'call_js_member': \"(call_expression function: (member_expression) @call_member)\",\n\n                # JSDoc: comment ngay tr∆∞·ªõc function/class/method\n                'docstring': \"\"\"\n                    (function_declaration (comment) @docstring) @func\n                    (class_declaration (comment) @docstring) @class\n                    (method_definition (comment) @docstring) @func\n                \"\"\"\n            },\n\n            # N·∫øu c·∫ßn TypeScript/TSX, b·∫°n c√≥ th·ªÉ copy block JS sang v√† ƒë·ªïi language_name t∆∞∆°ng ·ª©ng\n        }\n\n    def get_node_text(self, node) -> str:\n        \"\"\"Tr√≠ch xu·∫•t text an to√†n t·ª´ bytes.\"\"\"\n        snippet_bytes = self.code_content_bytes[node.start_byte:node.end_byte]\n        return snippet_bytes.decode(\"utf-8\", errors=\"ignore\")\n\n    def parse(self) -> Tuple[List[Dict], List[Dict]]:\n        tree = self.parser.parse(self.code_content_bytes)\n        root_node = tree.root_node\n\n        self._create_file_node()\n\n        # Phase 1: Definitions\n        self._run_query('class', root_node)\n        self._run_query('function', root_node)\n        self._run_query('method', root_node)\n        self._run_query('attribute', root_node)\n\n        # Phase 2: Docstrings\n        self._run_query('docstring', root_node)\n\n        # Phase 3: Dependencies\n        self._run_query('import_from', root_node)\n        self._run_query('import_simple', root_node)\n\n        # Phase 4: Calls ‚Äî ch·∫°y t·∫•t c·∫£ query b·∫Øt ƒë·∫ßu b·∫±ng 'call_'\n        for qname in list(self.queries.get(self.language_name, {}).keys()):\n            if qname.startswith('call_'):\n                self._run_query(qname, root_node)\n\n        return self.nodes, self.relationships\n\n    def _create_file_node(self):\n        if self.file_path not in self._processed_fqns:\n            self.nodes.append({\n                'fqn': self.file_path, 'type': 'File',\n                'name': os.path.basename(self.file_path), 'path': self.file_path\n            })\n            self._processed_fqns.add(self.file_path)\n\n    def _run_query(self, query_type: str, root_node):\n        query_str = self.queries.get(self.language_name, {}).get(query_type)\n        if not query_str:\n            return\n        try:\n            query = Query(self.language_obj, query_str)\n            cursor = QueryCursor(query)\n            captures = cursor.captures(root_node)\n            captures_by_name = captures\n        except Exception as e:\n            # In ra file + query ƒë·ªÉ debug nhanh\n            print(f\"L·ªói tree-sitter query (File: {self.file_path}, Query: {query_type}): {e}\")\n            return\n\n        handlers = {\n            'class': self._handle_class,\n            'function': self._handle_function,\n            'method': self._handle_method,\n            'attribute': self._handle_attribute,\n            'import_from': self._handle_import,\n            'import_simple': self._handle_import,\n\n            # CALLS\n            'call_py_identifier': self._handle_call,\n            'call_py_attribute': self._handle_call,\n            'call_java_name': self._handle_call,\n            'call_java_scoped': self._handle_call,\n            'call_js_identifier': self._handle_call,\n            'call_js_member': self._handle_call,\n\n            # Docstrings\n            'docstring': self._handle_docstring,\n        }\n        handler = handlers.get(query_type)\n        if handler:\n            handler(captures_by_name)\n\n    # ==========================\n    # Definition handlers\n    # ==========================\n    def _handle_class(self, caps: Dict):\n        class_name_nodes = caps.get('class_name', [])\n        for node in class_name_nodes:\n            class_name = self.get_node_text(node)\n            class_fqn = f\"{self.file_path}::{class_name}\"\n            if class_fqn in self._processed_fqns: continue\n\n            self.nodes.append({'fqn': class_fqn, 'type': 'Class', 'name': class_name})\n            self.relationships.append({'source_fqn': self.file_path, 'target_fqn': class_fqn, 'type': 'DEFINES_CLASS', 'properties': {}})\n\n            class_def_fqn = f\"DEF::{class_fqn}\"\n            class_node_def = node.parent\n            self.nodes.append({'fqn': class_def_fqn, 'type': 'ClassDefinition', 'code': self.get_node_text(class_node_def)[:Config.MAX_CODE_LENGTH]})\n            self.relationships.append({'source_fqn': class_fqn, 'target_fqn': class_def_fqn, 'type': 'HAS_DEFINITION', 'properties': {}})\n\n            self._processed_fqns.add(class_fqn)\n            self._processed_fqns.add(class_def_fqn)\n\n    def _handle_function(self, caps: Dict):\n        function_name_nodes = caps.get('function_name', [])\n        for node in function_name_nodes:\n            if self._is_inside_class(node): continue\n            func_name = self.get_node_text(node)\n            func_fqn = f\"{self.file_path}::{func_name}\"\n            if func_fqn in self._processed_fqns: continue\n\n            self.nodes.append({'fqn': func_fqn, 'type': 'Function', 'name': func_name})\n            self.relationships.append({'source_fqn': self.file_path, 'target_fqn': func_fqn, 'type': 'DEFINES_FUNCTION', 'properties': {}})\n\n            func_def_fqn = f\"DEF::{func_fqn}\"\n            func_node_def = node.parent\n            self.nodes.append({'fqn': func_def_fqn, 'type': 'FunctionDefinition', 'code': self.get_node_text(func_node_def)[:Config.MAX_CODE_LENGTH]})\n            self.relationships.append({'source_fqn': func_fqn, 'target_fqn': func_def_fqn, 'type': 'HAS_DEFINITION', 'properties': {}})\n\n            self._processed_fqns.add(func_fqn)\n            self._processed_fqns.add(func_def_fqn)\n\n    def _handle_method(self, caps: Dict):\n        method_nodes = caps.get('method_name', [])\n        class_nodes = caps.get('class_node', [])\n        if not method_nodes or not class_nodes: return\n\n        for method_node in method_nodes:\n            containing_class = None\n            for class_node in class_nodes:\n                if class_node.start_byte <= method_node.start_byte <= class_node.end_byte:\n                    containing_class = class_node\n                    break\n            if not containing_class: continue\n\n            class_name_node = containing_class.child_by_field_name('name')\n            if not class_name_node: continue\n\n            class_name = self.get_node_text(class_name_node)\n            class_fqn = f\"{self.file_path}::{class_name}\"\n\n            method_name = self.get_node_text(method_node)\n            method_fqn = f\"{class_fqn}::{method_name}\"\n            if method_fqn in self._processed_fqns: continue\n\n            self.nodes.append({'fqn': method_fqn, 'type': 'Method', 'name': method_name})\n            self.relationships.append({'source_fqn': class_fqn, 'target_fqn': method_fqn, 'type': 'HAS_METHOD', 'properties': {}})\n\n            method_def_fqn = f\"DEF::{method_fqn}\"\n            method_node_def = method_node.parent\n            self.nodes.append({'fqn': method_def_fqn, 'type': 'MethodDefinition', 'code': self.get_node_text(method_node_def)[:Config.MAX_CODE_LENGTH]})\n            self.relationships.append({'source_fqn': method_fqn, 'target_fqn': method_def_fqn, 'type': 'HAS_DEFINITION', 'properties': {}})\n\n            self._processed_fqns.add(method_fqn)\n            self._processed_fqns.add(method_def_fqn)\n\n    def _handle_attribute(self, caps: Dict):\n        attr_nodes = caps.get('attr_name', [])\n        class_nodes = caps.get('class_node', [])\n        if not attr_nodes or not class_nodes: return\n\n        for attr_node in attr_nodes:\n            containing_class = None\n            for class_node in class_nodes:\n                if class_node.start_byte <= attr_node.start_byte <= class_node.end_byte:\n                    containing_class = class_node\n                    break\n            if not containing_class: continue\n\n            class_name_node = containing_class.child_by_field_name('name')\n            if not class_name_node: continue\n\n            class_name = self.get_node_text(class_name_node)\n            class_fqn = f\"{self.file_path}::{class_name}\"\n\n            attr_name = self.get_node_text(attr_node)\n            attr_fqn = f\"{class_fqn}::{attr_name}\"\n            if attr_fqn in self._processed_fqns: continue\n\n            self.nodes.append({'fqn': attr_fqn, 'type': 'Attribute', 'name': attr_name})\n            self.relationships.append({'source_fqn': class_fqn, 'target_fqn': attr_fqn, 'type': 'HAS_ATTRIBUTE', 'properties': {}})\n\n            self._processed_fqns.add(attr_fqn)\n\n    def _handle_import(self, caps: Dict):\n        module_nodes = caps.get('module', [])\n        for node in module_nodes:\n            module_name = self.get_node_text(node).strip('\\'\"')\n            module_fqn = f\"MODULE::{module_name}\"\n            if module_fqn not in self._processed_fqns:\n                self.nodes.append({'fqn': module_fqn, 'type': 'Placeholder', 'name': module_name, 'category': 'module'})\n                self._processed_fqns.add(module_fqn)\n            self.relationships.append({'source_fqn': self.file_path, 'target_fqn': module_fqn, 'type': 'IMPORTS', 'properties': {'module_name': module_name}})\n\n    # ==========================\n    # CALLS handler (multi-language)\n    # ==========================\n    def _handle_call(self, caps: Dict):\n        \"\"\"\n        Chu·∫©n h√≥a t√™n h√†m ƒë∆∞·ª£c g·ªçi:\n        - Python: identifier ‚Üí l·∫•y tr·ª±c ti·∫øp; attribute ‚Üí l·∫•y ph·∫ßn cu·ªëi sau d·∫•u '.'\n        - JS: identifier ‚Üí tr·ª±c ti·∫øp; member_expression ‚Üí l·∫•y ph·∫ßn cu·ªëi (n·∫øu c√≥).\n        - Java: method_invocation name ‚Üí l·∫•y identifier/scoped_identifier cu·ªëi.\n        \"\"\"\n        # Gom t·∫•t c·∫£ capture key c√≥ th·ªÉ ch·ª©a t√™n g·ªçi\n        nodes = []\n        for key in ['call_name', 'call_attr_name', 'call_member', 'call_scoped']:\n            nodes.extend(caps.get(key, []))\n\n        for node in nodes:\n            raw_text = self.get_node_text(node).strip()\n\n            # Python\n            if self.language_name == 'python':\n                if node.type == 'identifier':\n                    call_name = raw_text\n                elif node.type == 'attribute':\n                    # attribute node tr·∫£ v·ªÅ to√†n c·ª•m \"obj.method\" ‚Üí l·∫•y ph·∫ßn cu·ªëi\n                    call_name = raw_text.split('.')[-1]\n                else:\n                    call_name = raw_text\n\n            # JavaScript\n            elif self.language_name == 'javascript':\n                if node.type == 'identifier':\n                    call_name = raw_text\n                elif node.type == 'member_expression':\n                    # C√≥ th·ªÉ d·∫°ng \"obj.method\" ho·∫∑c ph·ª©c t·∫°p h∆°n\n                    # Th∆∞·ªùng text ch·ª©a d·∫•u '.' ‚Üí l·∫•y ph·∫ßn cu·ªëi\n                    call_name = raw_text.split('.')[-1]\n                else:\n                    call_name = raw_text\n\n            # Java\n            elif self.language_name == 'java':\n                # method_invocation name: identifier/scoped_identifier\n                # N·∫øu scoped d·∫°ng \"pkg.Class.method\" ‚Üí l·∫•y ph·∫ßn cu·ªëi\n                call_name = raw_text.split('.')[-1]\n\n            else:\n                call_name = raw_text\n\n            caller_fqn = self._get_caller_context(node)\n            callee_fqn = f\"CALL::{call_name}\"\n\n            if callee_fqn not in self._processed_fqns:\n                self.nodes.append({'fqn': callee_fqn, 'type': 'Placeholder', 'name': call_name, 'category': 'call'})\n                self._processed_fqns.add(callee_fqn)\n\n            self.relationships.append({'source_fqn': caller_fqn, 'target_fqn': callee_fqn, 'type': 'CALLS', 'properties': {'call_name': call_name}})\n\n    # ==========================\n    # Docstring handlers\n    # ==========================\n    def _clean_docstring(self, text: str) -> str:\n        \"\"\"D·ªçn docstring/comment theo ng√¥n ng·ªØ b·∫±ng regex/strip ph√π h·ª£p.\"\"\"\n        if self.language_name == 'python':\n            # B√≥c ph·∫ßn gi·ªØa triple quotes ho·∫∑c single quotes\n            m = re.match(r'^[rRuU]*([\\'\"]{3})(.*?)([\\'\"]{3})$', text, re.S)\n            if m:\n                return m.group(2).strip()\n            m = re.match(r'^[rRuU]*([\\'\"])(.*?)([\\'\"])$', text, re.S)\n            if m:\n                return m.group(2).strip()\n            return text.strip()\n\n        elif self.language_name == 'java':\n            t = text.strip()\n            if t.startswith(\"/**\"):\n                t = t[3:]\n            if t.endswith(\"*/\"):\n                t = t[:-2]\n            lines = [line.lstrip('*').strip() for line in t.splitlines()]\n            return \"\\n\".join(lines).strip()\n\n        elif self.language_name in ['javascript', 'typescript', 'typescriptreact']:\n            t = text.strip()\n            if t.startswith(\"/**\"):\n                t = t[3:]\n            if t.endswith(\"*/\"):\n                t = t[:-2]\n            if t.startswith(\"//\"):\n                t = t[2:]\n            lines = [line.lstrip('*').strip() for line in t.splitlines()]\n            return \"\\n\".join(lines).strip()\n\n        return text.strip()\n\n    def _process_docstring_node(self, parent_node, doc_node):\n        name_node = parent_node.child_by_field_name('name')\n        if not name_node: return\n\n        parent_name = self.get_node_text(name_node)\n        parent_fqn = self._build_fqn_from_node(parent_node, parent_name)\n        if not parent_fqn: return\n\n        def_fqn = f\"DEF::{parent_fqn}\"\n        doc_fqn = f\"DOC::{parent_fqn}\"\n\n        if def_fqn not in self._processed_fqns:\n            return\n        if doc_fqn in self._processed_fqns:\n            return\n\n        doc_text = self._clean_docstring(self.get_node_text(doc_node))\n        if len(doc_text) < 5:\n            return\n\n        self.nodes.append({'fqn': doc_fqn, 'type': 'Documentation', 'text': doc_text})\n        self._processed_fqns.add(doc_fqn)\n        self.relationships.append({'source_fqn': def_fqn, 'target_fqn': doc_fqn, 'type': 'HAS_DOCUMENTATION', 'properties': {}})\n\n    def _handle_docstring(self, caps: Dict):\n        doc_nodes = caps.get('docstring', [])\n        func_parents = caps.get('func', [])\n        class_parents = caps.get('class', [])\n        parent_nodes = func_parents + class_parents\n        if not doc_nodes or not parent_nodes: return\n\n        # G√°n docstring ƒë√∫ng parent b·∫±ng kho·∫£ng byte\n        docs_by_parent: Dict[int, List] = {}\n        for dnode in doc_nodes:\n            for p in parent_nodes:\n                if p.start_byte <= dnode.start_byte <= p.end_byte:\n                    docs_by_parent.setdefault(p.id, []).append(dnode)\n                    break\n\n        for parent in parent_nodes:\n            candidates = docs_by_parent.get(parent.id, [])\n            if not candidates: continue\n\n            valid = None\n            if self.language_name == 'python':\n                body = parent.child_by_field_name('body')\n                if not body or body.type != 'block': continue\n                # statement ƒë·∫ßu ti√™n kh√¥ng ph·∫£i comment\n                first_stmt = None\n                for ch in body.children:\n                    if ch.type not in ('comment', 'line_comment', 'block_comment'):\n                        first_stmt = ch\n                        break\n                if not first_stmt: continue\n                if first_stmt.type == 'expression_statement' and first_stmt.child_count > 0:\n                    if first_stmt.children[0].type == 'string':\n                        valid = first_stmt.children[0]\n\n            elif self.language_name in ['java', 'javascript']:\n                body = parent.child_by_field_name('body')\n                body_start = body.start_byte if body else parent.end_byte\n                for d in candidates:\n                    if d.end_byte <= body_start:\n                        valid = d\n                        break\n\n            if valid:\n                self._process_docstring_node(parent, valid)\n\n    # ==========================\n    # Helpers\n    # ==========================\n    def _is_inside_class(self, node) -> bool:\n        current = node.parent\n        while current:\n            if current.type in ('class_definition', 'class_declaration'):\n                return True\n            current = current.parent\n        return False\n\n    def _get_caller_context(self, node) -> str:\n        current = node.parent\n        while current:\n            ntype = current.type\n            name_node = current.child_by_field_name('name')\n            if name_node and ntype in ('function_definition', 'method_declaration', 'function_declaration', 'method_definition'):\n                func_name = self.get_node_text(name_node)\n                class_parent = current.parent\n                while class_parent:\n                    if class_parent.type in ('class_definition', 'class_declaration'):\n                        cls_name_node = class_parent.child_by_field_name('name')\n                        if cls_name_node:\n                            cls = self.get_node_text(cls_name_node)\n                            return f\"{self.file_path}::{cls}::{func_name}\"\n                        break\n                    class_parent = class_parent.parent\n                return f\"{self.file_path}::{func_name}\"\n            if ntype in ('class_definition', 'class_declaration') and name_node:\n                cls = self.get_node_text(name_node)\n                return f\"{self.file_path}::{cls}\"\n            current = current.parent\n        return self.file_path\n\n    def _build_fqn_from_node(self, node, name: str) -> Optional[str]:\n        if node.type in ('function_definition', 'method_declaration', 'function_declaration', 'method_definition'):\n            current = node.parent\n            while current:\n                if current.type in ('class_definition', 'class_declaration'):\n                    cls_name_node = current.child_by_field_name('name')\n                    if cls_name_node:\n                        cls = self.get_node_text(cls_name_node)\n                        return f\"{self.file_path}::{cls}::{name}\"\n                    break\n                current = current.parent\n            return f\"{self.file_path}::{name}\"\n        elif node.type in ('class_definition', 'class_declaration'):\n            return f\"{self.file_path}::{name}\"\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:47:43.759460Z","iopub.execute_input":"2025-10-30T09:47:43.760454Z","iopub.status.idle":"2025-10-30T09:47:43.860010Z","shell.execute_reply.started":"2025-10-30T09:47:43.760420Z","shell.execute_reply":"2025-10-30T09:47:43.858961Z"},"scrolled":true},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# --- Ph·∫ßn 5: Code Enricher (LLM + Embeddings) ---\n# [ƒê√É C·∫¨P NH·∫¨T: Embed c·∫£ Docstring]\n\n\nclass CodeEnricher:\n    \"\"\"L√†m gi√†u nodes b·∫±ng LLM descriptions v√† embeddings cho Docstrings\"\"\"\n\n    def __init__(self, llm_client, encoder: SentenceTransformer):\n        self.llm = llm_client\n        self.encoder = encoder\n        print(\"‚úì CodeEnricher initialized\")\n\n    def enrich_nodes(self, nodes: List[Dict], relationships: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Pipeline l√†m gi√†u: Embed Docstrings, Generate LLM Descriptions + Embeddings\n        \"\"\"\n        print(\"\\nüîß Enriching nodes...\")\n\n        # T·∫°o map ƒë·ªÉ truy c·∫≠p nhanh node data b·∫±ng FQN\n        node_map = {n['fqn']: n for n in nodes}\n        # T·∫°o map ƒë·ªÉ t√¨m Definition t·ª´ Documentation FQN\n        doc_fqn_to_def_fqn = {r['target_fqn']: r['source_fqn']\n                              for r in relationships if r['type'] == 'HAS_DOCUMENTATION'}\n\n        doc_nodes_to_embed = []\n        def_nodes_for_llm = []\n\n        # Ph√¢n lo·∫°i c√°c node c·∫ßn x·ª≠ l√Ω\n        for node in nodes:\n             if node['type'] == 'Documentation' and 'embedding' not in node and node.get('text'):\n                  doc_nodes_to_embed.append(node)\n             elif node['type'].endswith('Definition'):\n                  # Ch·ªâ enrich n·∫øu ch∆∞a c√≥ description t∆∞∆°ng ·ª©ng\n                  desc_fqn = f\"DESC::{node['fqn']}\"\n                  if desc_fqn not in node_map:\n                       def_nodes_for_llm.append(node)\n\n        print(f\"Found {len(doc_nodes_to_embed)} documentation nodes to embed.\")\n        print(f\"Found {len(def_nodes_for_llm)} definition nodes for LLM description.\")\n\n        # 1. Embed Docstrings\n        doc_embed_count = self._embed_documentation(doc_nodes_to_embed)\n\n        # 2. Generate LLM Descriptions + Embeddings\n        llm_desc_count = self._generate_llm_descriptions(def_nodes_for_llm, node_map, doc_fqn_to_def_fqn, nodes, relationships)\n\n        print(f\"‚úì Enrichment complete! Embedded {doc_embed_count} docstrings, added {llm_desc_count} LLM descriptions.\")\n        return nodes, relationships\n\n    def _embed_documentation(self, doc_nodes: List[Dict]) -> int:\n        \"\"\"T·∫°o embedding cho c√°c node Documentation\"\"\"\n        count = 0\n        iterable = doc_nodes\n        if Config.ENABLE_PROGRESS_BAR:\n            iterable = tqdm(doc_nodes, desc=\"  Embedding Docstrings\", unit=\"doc\")\n\n        texts_to_embed = [node['text'] for node in iterable if node.get('text')]\n        if not texts_to_embed: return 0\n\n        try:\n             # Embed h√†ng lo·∫°t\n             embeddings = self.encoder.encode(texts_to_embed, batch_size=32, show_progress_bar=False) # T·∫Øt progress bar c·ªßa encoder\n\n             # G√°n embedding l·∫°i cho c√°c node\n             idx = 0\n             for node in iterable:\n                  if node.get('text'):\n                       if idx < len(embeddings):\n                            node['embedding'] = embeddings[idx].tolist()\n                            count += 1\n                            idx += 1\n        except Exception as e:\n             print(f\"‚ö† Error embedding documentation: {e}\")\n\n        return count\n\n    def _generate_llm_descriptions(self, def_nodes: List[Dict], node_map: Dict, doc_fqn_to_def_fqn: Dict, nodes_list: List[Dict], relationships_list: List[Dict]) -> int:\n        \"\"\"T·∫°o m√¥ t·∫£ LLM v√† embedding cho c√°c node Definition\"\"\"\n        count = 0\n        iterable = def_nodes\n        if Config.ENABLE_PROGRESS_BAR:\n            iterable = tqdm(def_nodes, desc=\"  Generating LLM Descriptions\", unit=\"def\")\n\n        for def_node in iterable:\n            def_fqn = def_node['fqn']\n            code = def_node.get('code', '')\n            if not code: continue\n\n            # T√¨m docstring t·ª´ node Documentation li√™n k·∫øt\n            docstring_text = \"\"\n            for doc_fqn, linked_def_fqn in doc_fqn_to_def_fqn.items():\n                if linked_def_fqn == def_fqn:\n                    doc_node = node_map.get(doc_fqn)\n                    if doc_node and 'text' in doc_node:\n                        docstring_text = doc_node['text']\n                        break\n\n            original_fqn = def_fqn.replace(\"DEF::\", \"\")\n            original_node_type = node_map.get(original_fqn, {}).get('type', \"Code Snippet\")\n\n            # Generate description\n            description = self._generate_description(code, docstring_text, original_node_type)\n            if not description: continue\n\n            # Generate embedding\n            embedding = self._generate_embedding(description)\n\n            # T·∫°o Node GeneratedDescription\n            desc_fqn = f\"DESC::{def_fqn}\"\n            if not any(n['fqn'] == desc_fqn for n in nodes_list):\n                 desc_node = {'fqn': desc_fqn, 'type': 'GeneratedDescription',\n                              'description': description, 'embedding': embedding}\n                 nodes_list.append(desc_node)\n\n                 # T·∫°o Relationship HAS_DESCRIPTION\n                 if not any(r['source_fqn'] == def_fqn and r['type'] == 'HAS_DESCRIPTION' for r in relationships_list):\n                     relationships_list.append({'source_fqn': def_fqn, 'target_fqn': desc_fqn,\n                                                'type': 'HAS_DESCRIPTION', 'properties': {}})\n                     count += 1\n        return count\n\n\n    def _generate_description(self, code: str, docstring: str, node_type: str) -> str:\n        \"\"\"Generate description v·ªõi LLM\"\"\"\n        prompt = f\"\"\"Analyze the following {node_type}. Provide a concise (1-2 sentences) description explaining its primary function or purpose. Focus on *what* it does.\n\nDOCSTRING:\n{docstring if docstring else \"N/A\"}\n\nCODE:\n{code[:Config.MAX_CODE_LENGTH]}\n\nCONCISE DESCRIPTION:\"\"\"\n        response = self.llm.invoke([HumanMessage(content=prompt)]) # Gi·∫£m max_tokens\n        description = response.content\n        return description.strip().replace(\"\\n\", \" \") # X√≥a xu·ªëng d√≤ng\n\n    def _generate_embedding(self, text: str) -> List[float]:\n        \"\"\"Generate embedding v·ªõi Sentence Transformer\"\"\"\n        try:\n            embedding = self.encoder.encode(text, convert_to_numpy=True)\n            return embedding.tolist()\n        except Exception as e:\n             print(f\"‚ö† Error generating embedding for text '{text[:50]}...': {e}\")\n             return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:31:12.181891Z","iopub.execute_input":"2025-10-30T09:31:12.182286Z","iopub.status.idle":"2025-10-30T09:31:12.213559Z","shell.execute_reply.started":"2025-10-30T09:31:12.182259Z","shell.execute_reply":"2025-10-30T09:31:12.212144Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# --- Ph·∫ßn 6: Ch·∫°y Pipeline Giai ƒëo·∫°n 1 (ƒê√£ C·∫£i Thi·ªán) ---\n\ndef main():\n    \"\"\"H√†m ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline Giai ƒëo·∫°n 1\"\"\"\n    start_time = time.time() # B·∫Øt ƒë·∫ßu b·∫•m gi·ªù\n\n    try:\n        # 1. Kh·ªüi t·∫°o\n        print(\"--- Initializing ---\")\n        secrets = UserSecretsClient()\n        neo4j_uri = secrets.get_secret(Config.NEO4J_URI_SECRET)\n        neo4j_user = secrets.get_secret(Config.NEO4J_USER_SECRET)\n        neo4j_password = secrets.get_secret(Config.NEO4J_PASSWORD_SECRET)\n        openai_api_key = secrets.get_secret(Config.OPENAI_API_KEY_SECRET)\n        graph_constructor = Neo4jGraphConstructor(neo4j_uri, neo4j_user, neo4j_password)\n        llm = ChatOpenAI(openai_api_key=openai_api_key, model=Config.OPENAI_MODEL)\n        encoder = SentenceTransformer(Config.ENCODER_MODEL)\n        enricher = CodeEnricher(llm, encoder)\n        repo_parser = RepositoryParser(Config.REPO_ROOT_DIR, LANGUAGE_MAP) # Truy·ªÅn LANGUAGE_MAP\n\n        # 2. Chu·∫©n b·ªã DB\n        graph_constructor.clear_database()\n        graph_constructor.create_constraints_and_indexes()\n\n        # 3. Parse Repository\n        all_nodes, all_relationships = repo_parser.parse_repository()\n\n        # 4. Enrich Nodes\n        all_nodes, all_relationships = enricher.enrich_nodes(all_nodes, all_relationships)\n\n        # 5. Ingest Data\n        graph_constructor.ingest_data(all_nodes, all_relationships)\n\n        # 6. Verify Graph (Optional but recommended)\n        graph_constructor.verify_graph()\n\n        # 7. Close Connection\n        graph_constructor.close()\n\n        end_time = time.time() # K·∫øt th√∫c b·∫•m gi·ªù\n        print(f\"\\n--- PHASE 1 COMPLETE (Improved) ---\")\n        print(f\"Total time: {end_time - start_time:.2f} seconds\")\n        print(\"Knowledge Graph is built, enriched, and indexed.\")\n\n    except Exception as e:\n        print(f\"\\n--- üí• PIPELINE FAILED ---\")\n        print(f\"Error: {e}\")\n        # ƒê·∫£m b·∫£o ƒë√≥ng k·∫øt n·ªëi n·∫øu c√≥ l·ªói\n        if 'graph_constructor' in locals() and graph_constructor.driver:\n            try:\n                graph_constructor.close()\n            except: pass # Ignore errors during closing on failure\n\n# --- Ch·∫°y h√†m main ---\nimport time\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:47:50.471912Z","iopub.execute_input":"2025-10-30T09:47:50.472287Z","iopub.status.idle":"2025-10-30T09:51:52.384742Z","shell.execute_reply.started":"2025-10-30T09:47:50.472263Z","shell.execute_reply":"2025-10-30T09:51:52.383392Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"--- Initializing ---\n‚úì Connected to Neo4j AuraDB\n‚úì CodeEnricher initialized\n\nüóëÔ∏è  Clearing database...\n‚úì Database cleared\n\nüîß Creating constraints and indexes...\n‚úì Constraints and indexes created\n\nüìÇ Parsing repository: /kaggle/input/chatbot/chatbotai_v1-main\nFound 25 files to parse after filtering\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Parsing files:   0%|          | 0/25 [00:00<?, ?file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"697473fee2e9459ba37c9b60b062fd71"}},"metadata":{}},{"name":"stdout","text":"  üü° Skipping empty file: /kaggle/input/chatbot/chatbotai_v1-main/backend/__init__.py\n  üü° Skipping empty file: /kaggle/input/chatbot/chatbotai_v1-main/backend/database/__init__.py\n  üü° Skipping empty file: /kaggle/input/chatbot/chatbotai_v1-main/backend/database/schemas/__init__.py\n  üü° Skipping empty file: /kaggle/input/chatbot/chatbotai_v1-main/backend/routers/___init__.py\n  üü° Skipping empty file: /kaggle/input/chatbot/chatbotai_v1-main/backend/utils/__init__.py\n\n‚úì Parsing complete!\n  - Total nodes extracted: 1021\n  - Total relationships extracted: 2069\n\nüîß Enriching nodes...\nFound 50 documentation nodes to embed.\nFound 139 definition nodes for LLM description.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  Embedding Docstrings:   0%|          | 0/50 [00:00<?, ?doc/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf4ce84267b4b038e24e80655e086c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  Generating LLM Descriptions:   0%|          | 0/139 [00:00<?, ?def/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91981e266a054ff3b763722b9ad43ab3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dce4c3ae3a245c497679c7161d82e3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b2bc41b9c84c93938da6ba9cd50ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83466dab42ce4feba01f7db640e31090"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9c43b2bea994807be01ccc2e62e1cf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce0b3baba5ac454a8d8dd892c0a38241"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931ee6b55ee24c91b278ae207f7a3e9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd9c674f7574fb5ab4777277f2abea0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44c3a8f0ae349e48690fcbb0a919457"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30a1edcfd046479ea76b9921f7ceab9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1d0fe5e2e654294a56746c869cd797e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09b2826c05224c5c9380a2b719899931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68390fa16b3641389a38257014c61564"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43924c1cd0074fd8b568f946a6d301a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"608c31c2740949bc9d34924478f2d574"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eea71d4518ff4d03a1dedfc892e1f6ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7102368bd43849f38fea2b90741670c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"169787d355ac49bebabfdce204b25b24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e18ff2dae946f9ae0ce84427231caa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe038f0375244b097309683052653bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"801a7c85b6eb42ecac6833fd8f36a1db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7312f89e85249579fc60c78e25c25e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59e77d3e3dde47e788480bea746922ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c214614be5546069b2b9d535e41ad08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791cbf5c8d944a039ac7b54af035b63b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9f0e1c120f9487ba8e52b86d175ae06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1fdb9289a694e5aac69d500271262b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ef3b68f5a0b4ff597b1b62297b4c26c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f10089fb714b0196e87839da4a7e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5428a891fe2146fc8cd81e5d67c9668b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3c507749dfd411892ef345316a73d94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458972f29b254c53a97d70d6aae31084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc5514d4bd094f21a3f6e2a331288965"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ed783b78784a279bc7449f2246001f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14e466e452c344dcb93b4b3d2142f2f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"569c2e859c0d4fb395402fc8c4e21199"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ec6fb908d5492ca1f5a215f7866af7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"713e13cf9dae4f94b1802779d6c8511c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86961ddb6c4b4766bd14e749e43e06e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8262f84e6f24742a4494f27ece8c1db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74842722f126476eb37736001a495b11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c58c24802ce04ef382c0ea1f7d06e7e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a07e84213c3d4507a5b1afb601d795e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d1849cd7c04aac8fd33310f2c39930"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ac74b298f340efaaf14f813e7e3548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac56cdd87424453194a2e8c1514ba4e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19875638359741148d4160395162c4e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a79280ca6d824a45a400dea545d53d8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc7061c3a64e41f69433ecaa3cf0f76d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e8639457f64a62b8f793dfbdc2f4ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1bc422c42dd4867be82f90050920c29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5705a7eb851e41b28213ae581bc07e3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"905907bd67a541e79f61737623fed955"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df3239c421f14903ac6fa52b5aa1aec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86aecfa75e3f4c90b977d55f1d6c4f70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"887fe68eb8634f5699a1bb003f6d14e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3d3e222368f4c978b1d35fabe1c5de7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4787be0072fd43ce83fcb251acf91d02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8539505fce88402f8cf7323033566571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef6d718ecc9400595963c99a15029db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19c3e5d4d7424b0d926ffb28f029d9f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80ed3f745cc244a88f93c1ead84a2bdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5d4585041d74783a175962854269548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"602bc19284b84a1baaf74f441a941795"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4e190d457964c5fb3437fa4824b6516"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16372a0a269c4abe832522c139c00041"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5caebaaaee2e4b87bb424fdd66f0d720"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc4c19e25874d55b3e2bc02c104adc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0b061c2be2048399bbc53227d660a9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90be2134b09a4b3890e319bedb68353a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8e8b0c9ee4442889cb7c15f7e80ae03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"263588a82bef431e9c0b33ff61220aa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ecf8dbb4ca4272a3b516ed836de01b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4ee471862cb4324a28bf1f4e5aca7c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2688f55bf6b4c319cd6ce5444fcccc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e460ced6347c47e6af20840005921a8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cb9d775dff54ee89b2a673c063c59a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3fd2ee92194a6cae460a184b8b06b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b5f183862f48308c9c8e8c812492a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d5ee8ccd85549f69a82e95c22113050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c3c971d656f409490223902f4c7ee45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6e9ba25d564b7cb10f8e74c52bc52a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"566ecaf7fb344c1abffa58840ba594ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f8c0894e90433f8462eeaa78ddd4fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"464917c7ab854b5fbb4c4dd06dc359a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec77f37dd6874f7baaf32f1bdeab773b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"065b09c546334e8e95ebbb3a2885030b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0969978867344e6685f671aa896ea079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176cfa09fa994c4e8eec3265c1d84096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b036b29e5a924d59a00bfe7c4829d195"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f053538f9f664e53a49a9b107c03c275"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541bcb8b1ca54133bc7f672378bbd9a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6524976c8d54c1fac41f784778a2a06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3980fe97aa12465da4f13f082265855c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64dc188f70f9484b89119634769a798e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203a92edf4e04843a434f0a806c47322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20669af1ad6a4dbeb32b08f83a448052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f530c8dc2d0c49a1ad6dd85bad282794"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6246cd30207e4353a27e348d8307459c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1622fa5eb85b41f991fb1408b6b7108f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701830c3daea4909a1baa57e2b269347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8981469ba2d24c44bf069adfff75750d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc791d4af5ef4add986bc1250aaa1bde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21775c28d6924bd0afc8a9db8180e5e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd186f6f489e45a595ed48699f65fbae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26fac5bec8ac4f31b04ed5ca60e7fe5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"025740933b914d56a0685221aadbb0e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b70f5c57ac4f4b8488c6b8a323ed45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce6031abaf0343f1a13b19f848f38fe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"336dc6c5c4764f97bd3110da02f9a2d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3614e357950409dbbf9b3aa539e4fa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd238fd018b1412ba8b7f251267efd6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"870f93a7a8194fb1b1cbf457bda772c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6952a812cd3045f6a6886a1424518323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af3b9c60cb86413dbda0f2d165bce65e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2a4f2096eea4cb4bc8f1ee1edec8871"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6556a233939a4d5f9cec5070ce5efa0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d6c289398540a6b12b936795f47a09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7945fac0be594b159c025c6d7f801262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"161f1cf0c29e4995bd71c30f0fef6897"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fec7aaf2a8545d5aed43b460721b455"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb68c158cf924965a08fcfc584e909e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0fe0504195e452097b2ff3d2551438e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be76acc5924e4abbbc5ba0d7170e4f01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae07b3bb243496096b4099052a3c411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d60918be15b42689332c5913fdbe934"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c580f0b29147e7b2ead43ae244fdc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a1d12b0d9fd42c1bb9efca2b722771f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1244ce2cff53464588f7665dc701ae50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"359f2b8b0cfb48f3a426c4c24ebe0c46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d65845cfd46448dabc45f67f0c28c789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f30ba0c03d6146cba4fe62a7366e3c63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"540e30ebd01b4b32ad5639e0431dc23d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba73b0c362f480b94b694cbea10d3bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e3d55b428a44a63a51763c7ec39a29a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c33305fda5428e89a00db202aa0f04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b4e5e84e03148fe97c26aafc720edbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed446910b27943f9945dd1b5553f0cc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b03ecf8f0dc44b8eb9a0f27b9386c0c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1708fbef4ef84231a6b739279ce5afc2"}},"metadata":{}},{"name":"stdout","text":"‚úì Enrichment complete! Embedded 50 docstrings, added 139 LLM descriptions.\n\nüì• Ingesting data to Neo4j...\n  - Nodes to ingest: 1160\n  - Relationships to ingest: 2208\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  Ingesting Nodes:   0%|          | 0/3 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61547442391648d2a4c94ff6461aa6d5"}},"metadata":{}},{"name":"stdout","text":"  ‚úì Processed 1160 nodes (actual inserts depend on MERGE)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  Ingesting Relationships:   0%|          | 0/5 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7aa9c27004a4b36863b7e2a6682962c"}},"metadata":{}},{"name":"stdout","text":"  ‚úì Processed 2208 relationships (actual inserts depend on MERGE)\n‚úì Data ingestion complete\n\nüìä Graph Statistics:\n\n  Node Types (excluding :Node):\n    Placeholder: 227\n    Node: 159\n    GeneratedDescription: 139\n    Attribute: 127\n    FunctionDefinition: 121\n    Documentation: 50\n    ClassDefinition: 18\n\n  Relationship Types:\n    CALLS: 793\n    IMPORTS: 139\n    HAS_DEFINITION: 139\n    HAS_DESCRIPTION: 139\n    HAS_ATTRIBUTE: 127\n    DEFINES_FUNCTION: 121\n    HAS_DOCUMENTATION: 50\n    DEFINES_CLASS: 18\n\n  Total Nodes: 841\n  Total Relationships: 1526\n‚úì Disconnected from Neo4j\n\n--- PHASE 1 COMPLETE (Improved) ---\nTotal time: 241.90 seconds\nKnowledge Graph is built, enriched, and indexed.\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"# Phase 2: Hybrid Code Retrieval","metadata":{}},{"cell_type":"code","source":"import json\nclass CodeRetriever:\n    def __init__(self, graph_constructor, llm, encoder):\n        \"\"\"\n        Initializes the retriever with necessary components.\n        Args:\n            graph_constructor: Instance of Neo4jGraphConstructor (or similar) to run Cypher.\n            llm: Language model instance (e.g., ChatOpenAI) for entity extraction.\n            encoder: SentenceTransformer instance for creating embeddings.\n        \"\"\"\n        self.graph = graph_constructor\n        self.llm = llm\n        self.encoder = encoder\n        print(\"Code Retriever initialized.\")\n\n    # === Step 2.1: Query Processing ===\n    def _get_query_vector(self, query: str) -> list[float]:\n        \"\"\"Encodes the user query into a vector using the sentence transformer.\"\"\"\n        print(f\"\\n1a. Encoding query: '{query}'\")\n        try:\n            return self.encoder.encode(query, convert_to_numpy=True).tolist()\n        except Exception as e:\n            print(f\"  Error encoding query: {e}\")\n            return []\n\n    def _identify_entities(self, query: str) -> list[str]:\n        \"\"\"Uses LLM to identify potential code entities (names) mentioned in the query.\"\"\"\n        print(f\"\\n1b. Identifying entities in query: '{query}'\")\n        # Prompt asking for specific code artifacts like function/class names\n        prompt = f\"\"\"\n        Analyze the following user query about a software project. Identify any specific function names,\n        class names, method names, variable names, or module names mentioned.\n        Extract only these names. Do not include generic terms like 'function', 'class', 'file', etc.\n        Return the result ONLY as a JSON list of strings. If no specific names are found, return an empty list [].\n\n        User Query: \"{query}\"\n\n        JSON List of Names:\n        \"\"\"\n        try:\n            response = self.llm.invoke(prompt)\n            # Attempt to parse the JSON list from the LLM response\n            content = response.content.strip()\n            # Handle potential markdown code block ```json ... ```\n            if content.startswith(\"```json\"):\n                content = content[7:]\n            if content.endswith(\"```\"):\n                content = content[:-3]\n            content = content.strip()\n\n            entities = json.loads(content)\n            # Validate the result is a list of strings\n            if isinstance(entities, list) and all(isinstance(e, str) for e in entities):\n                 print(f\"  Identified entities: {entities}\")\n                 return entities\n            else:\n                print(f\"  Warning: LLM did not return a valid JSON list of strings. Response: {content}\")\n                return []\n        except json.JSONDecodeError:\n             print(f\"  Warning: LLM response was not valid JSON. Response: {response.content}\")\n             return []\n        except Exception as e:\n            print(f\"  Error identifying entities: {e}\")\n            return []\n\n    # === Step 2.2: Initial Retrieval (Hybrid Search) ===\n    def _initial_retrieval(self, query_vector: list[float], entities: list[str], top_k: int = 5) -> set[str]:\n        \"\"\"\n        Performs hybrid search (Vector + Full-text) against the Neo4j graph.\n        Returns a set of unique FQNs (Fully Qualified Names) of the initially retrieved code nodes.\n        \"\"\"\n        initial_fqns = set()\n\n        # --- Semantic Search (Vector Index) ---\n        if query_vector:\n            print(f\"\\n2a. Performing Semantic Search (top {top_k})...\")\n            # Uses the 'descriptions' vector index created in Phase 1\n            cypher_vector_search = \"\"\"\n                CALL db.index.vector.queryNodes('descriptions', $top_k, $query_vector) YIELD node AS descNode, score\n                // Navigate from description -> definition -> actual code node\n                MATCH (codeNode)-[:HAS_DEFINITION]->(defNode)-[:HAS_DESCRIPTION]->(descNode)\n                WHERE codeNode:Function OR codeNode:Method OR codeNode:Class // Focus on core code elements\n                RETURN codeNode.fqn AS fqn, score\n                ORDER BY score DESC\n            \"\"\"\n            try:\n                results = self.graph.run_cypher_query(cypher_vector_search,\n                                                     params={'top_k': top_k, 'query_vector': query_vector})\n                if results:\n                    found_fqns = {record['fqn'] for record in results}\n                    initial_fqns.update(found_fqns)\n                    print(f\"  Semantic Search added {len(found_fqns)} FQNs.\")\n            except Exception as e:\n                print(f\"  Error during Semantic Search: {e}\")\n\n        # --- Full-Text Search (Keyword Index) ---\n        if entities:\n            print(f\"\\n2b. Performing Full-Text Search for entities: {entities} (top {top_k})...\")\n            # Build a query string suitable for Neo4j full-text (simple OR logic)\n            # Escape special characters if necessary, or use parameterized approach if supported\n            ft_query_string = \" OR \".join(f'\"{entity}\"' for entity in entities) # Wrap entities in quotes if needed\n\n            # Uses the 'names' full-text index created in Phase 1\n            cypher_ft_search = \"\"\"\n                CALL db.index.fulltext.queryNodes('names', $query_string, {limit: $top_k}) YIELD node AS codeNode, score\n                WHERE codeNode:Function OR codeNode:Method OR codeNode:Class OR codeNode:Attribute // Match indexed types\n                RETURN codeNode.fqn AS fqn, score\n                ORDER BY score DESC\n            \"\"\"\n            try:\n                results = self.graph.run_cypher_query(cypher_ft_search,\n                                                     params={'query_string': ft_query_string, 'top_k': top_k})\n                if results:\n                    found_fqns = {record['fqn'] for record in results}\n                    initial_fqns.update(found_fqns)\n                    print(f\"  Full-Text Search added {len(found_fqns)} unique FQNs.\")\n            except Exception as e:\n                print(f\"  Error during Full-Text Search: {e}\")\n\n        print(f\"\\n=> Initial retrieval combined set size: {len(initial_fqns)}\")\n        return initial_fqns\n\n    # === Step 2.3: Graph Traversal ===\n    def _retrieve_n_hop_subgraph(self, start_fqns: set[str], num_hops: int = 2) -> tuple[list[dict], list[dict]]:\n        \"\"\"\n        Retrieves the n-hop subgraph around the starting FQNs using APOC.\n        Returns lists of node dictionaries and relationship dictionaries.\n        \"\"\"\n        nodes_list = []\n        relationships_list = []\n        if not start_fqns:\n            print(\"\\n3. Graph Traversal skipped: No start nodes.\")\n            return nodes_list, relationships_list\n\n        print(f\"\\n3. Retrieving {num_hops}-hop subgraph from {len(start_fqns)} start nodes...\")\n\n        # --- S·ª¨A L·ªñI C√ö PH√ÅP CYPHER ---\n        # Ch·∫°y APOC cho m·ªói start node, sau ƒë√≥ UNWIND v√† COLLECT DISTINCT\n        cypher_n_hop = \"\"\"\n            MATCH (startNode) WHERE startNode.fqn IN $start_fqns\n            CALL apoc.path.subgraphAll(startNode, {\n                maxLevel: $num_hops,\n                relationshipFilter: \"DEFINES_CLASS>|DEFINES_FUNCTION>|<HAS_METHOD|<HAS_DEFINITION>|<HAS_ATTRIBUTE|<IMPORTS>|<CALLS>|<HAS_DESCRIPTION\"\n            }) YIELD nodes, relationships\n            // Unwind lists returned for each start node\n            UNWIND nodes AS n\n            UNWIND relationships AS r\n            // Collect distinct nodes and relationships across all subgraphs\n            RETURN collect(DISTINCT n) AS distinctNodes,\n                   collect(DISTINCT r) AS distinctRelationships\n        \"\"\"\n        # ---------------------------------\n\n        try:\n            results = self.graph.run_cypher_query(cypher_n_hop,\n                                                 params={'start_fqns': list(start_fqns), 'num_hops': num_hops})\n\n            # Ki·ªÉm tra k·∫øt qu·∫£ c·∫©n th·∫≠n h∆°n\n            if results and results[0] and results[0].get('distinctNodes') is not None and results[0].get('distinctRelationships') is not None:\n                 all_nodes_raw = results[0]['distinctNodes']\n                 all_rels_raw = results[0]['distinctRelationships']\n\n                 # X·ª≠ l√Ω k·∫øt qu·∫£ (gi·ªØ nguy√™n logic chuy·ªÉn ƒë·ªïi)\n                 nodes_map = {n.id: {\"id\": n.id, \"labels\": list(n.labels), **dict(n.items())} for n in all_nodes_raw}\n                 nodes_list = list(nodes_map.values())\n\n                 for rel in all_rels_raw:\n                     start_node_data = nodes_map.get(rel.start_node.id)\n                     end_node_data = nodes_map.get(rel.end_node.id)\n                     if start_node_data and end_node_data:\n                         relationships_list.append({\n                             \"id\": rel.id,\n                             \"type\": rel.type,\n                             \"startNodeId\": rel.start_node.id,\n                             \"endNodeId\": rel.end_node.id,\n                             \"startNodeFqn\": start_node_data.get('fqn'),\n                             \"endNodeFqn\": end_node_data.get('fqn'),\n                             \"properties\": dict(rel.items())\n                         })\n\n                 print(f\"  Retrieved subgraph with {len(nodes_list)} nodes and {len(relationships_list)} relationships.\")\n            else:\n                 # In r√µ h∆°n n·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ ho·∫∑c k·∫øt qu·∫£ r·ªóng\n                 print(f\"  Graph traversal query executed but returned no distinct nodes or relationships. Result: {results}\")\n\n\n        except Exception as e:\n            print(f\"  Error during N-hop retrieval: {e}\")\n\n        return nodes_list, relationships_list\n\n\n    # === Step 2.4: Filtering Sub-graph ===\n    def _filter_subgraph_by_similarity(self, nodes: list[dict], relationships: list[dict], query_vector: list[float], top_k_filter: int = 20) -> tuple[list[dict], list[dict]]:\n        \"\"\"\n        Filters the subgraph based on semantic similarity to the query.\n        Prioritizes GeneratedDescription nodes and keeps directly connected structural nodes.\n        \"\"\"\n        filtered_nodes = []\n        filtered_relationships = []\n        if not nodes or not query_vector or top_k_filter <= 0:\n            print(\"\\n4. Filtering skipped: No nodes, query vector, or filter size.\")\n            return nodes, relationships # Return original if no filtering needed\n\n        print(f\"\\n4. Filtering subgraph by similarity (aiming for ~{top_k_filter} most relevant nodes)...\")\n\n        try:\n            # Calculate similarity for nodes with embeddings\n            node_similarities = []\n            for node in nodes:\n                if node.get('embedding') and node.get('labels') and 'GeneratedDescription' in node.get('labels'):\n                    node_vector = node['embedding']\n                    # Cosine Similarity using numpy\n                    vec1 = np.array(query_vector)\n                    vec2 = np.array(node_vector)\n                    # Handle potential zero vectors\n                    norm1 = np.linalg.norm(vec1)\n                    norm2 = np.linalg.norm(vec2)\n                    if norm1 == 0 or norm2 == 0:\n                        similarity = 0.0\n                    else:\n                        similarity = np.dot(vec1, vec2) / (norm1 * norm2)\n                    node_similarities.append((node['id'], similarity, node.get('fqn'))) # Store node ID and FQN\n\n            # Sort by similarity (highest first)\n            node_similarities.sort(key=lambda item: item[1], reverse=True)\n\n            # Get IDs of top-k description nodes\n            top_desc_node_ids = {node_id for node_id, score, fqn in node_similarities[:top_k_filter]}\n            print(f\"  Identified {len(top_desc_node_ids)} top description nodes.\")\n\n            # Find structural nodes (Code, Definition, File) connected to these top descriptions\n            connected_structural_node_ids = set()\n            node_id_to_data = {node['id']: node for node in nodes} # Map ID to full node data\n\n            for rel in relationships:\n                is_connected_to_top_desc = False\n                other_node_id = None\n                connected_node_type = None\n\n                if rel['endNodeId'] in top_desc_node_ids and rel['type'] == 'HAS_DESCRIPTION':\n                    is_connected_to_top_desc = True\n                    other_node_id = rel['startNodeId'] # This should be a Definition node\n                    connected_node_data = node_id_to_data.get(other_node_id)\n                    if connected_node_data:\n                         connected_node_type = 'Definition' # Mark type for clarity\n\n                # Add logic here if descriptions can link from code nodes directly (based on schema)\n\n                if is_connected_to_top_desc and other_node_id:\n                     connected_structural_node_ids.add(other_node_id)\n                     # Now, find the actual code node linked to this definition node\n                     for r_inner in relationships:\n                         if r_inner['endNodeId'] == other_node_id and r_inner['type'] == 'HAS_DEFINITION':\n                              code_node_id = r_inner['startNodeId']\n                              connected_structural_node_ids.add(code_node_id)\n                              # Also add the file containing the code node\n                              code_node_data = node_id_to_data.get(code_node_id)\n                              if code_node_data:\n                                   for r_file in relationships:\n                                       if r_file['endNodeId'] == code_node_id and r_file['type'] in ('DEFINES_CLASS', 'DEFINES_FUNCTION'):\n                                            connected_structural_node_ids.add(r_file['startNodeId'])\n                                       elif r_file['startNodeId'] == code_node_id and r_file['type'] == 'HAS_METHOD': # Method links from class\n                                            class_node_id = r_file['startNodeId']\n                                            for r_file_class in relationships:\n                                                 if r_file_class['endNodeId'] == class_node_id and r_file_class['type'] == 'DEFINES_CLASS':\n                                                      connected_structural_node_ids.add(r_file_class['startNodeId'])\n\n\n            print(f\"  Found {len(connected_structural_node_ids)} structural nodes connected to top descriptions.\")\n\n            # Combine top description nodes and connected structural nodes\n            final_node_ids = top_desc_node_ids.union(connected_structural_node_ids)\n\n            # Filter nodes and relationships based on the final set of IDs\n            filtered_nodes = [node for node in nodes if node['id'] in final_node_ids]\n            filtered_relationships = [rel for rel in relationships if rel['startNodeId'] in final_node_ids and rel['endNodeId'] in final_node_ids]\n\n            print(f\"=> Filtered subgraph to {len(filtered_nodes)} nodes and {len(filtered_relationships)} relationships.\")\n\n        except ImportError:\n            print(\"  Numpy not installed. Skipping similarity filtering.\")\n            return nodes, relationships # Return unfiltered if numpy unavailable\n        except Exception as e:\n            print(f\"  Error during filtering: {e}\")\n            return nodes, relationships # Return unfiltered on error\n\n        return filtered_nodes, filtered_relationships\n\n    # === Main Retrieval Function ===\n    def retrieve(self, query: str, initial_top_k: int = 5, num_hops: int = 2, filter_top_k: int = 20) -> tuple[list[dict], list[dict]]:\n        \"\"\"\n        Executes the full retrieval pipeline: Process Query -> Initial Retrieval -> Graph Traversal -> Filter.\n        Returns the filtered subgraph (list of node dicts, list of relationship dicts).\n        \"\"\"\n        print(f\"\\n--- Starting Retrieval Pipeline for Query: '{query}' ---\")\n        # 1. Process Query\n        query_vector = self._get_query_vector(query)\n        entities = self._identify_entities(query)\n\n        # 2. Initial Retrieval (Hybrid)\n        initial_fqns = self._initial_retrieval(query_vector, entities, initial_top_k)\n        if not initial_fqns:\n            print(\"Pipeline stopped: Initial retrieval failed to find any relevant nodes.\")\n            return [], []\n\n        # 3. Graph Traversal (N-Hop Expansion)\n        subgraph_nodes, subgraph_rels = self._retrieve_n_hop_subgraph(initial_fqns, num_hops)\n        if not subgraph_nodes:\n            print(\"Pipeline stopped: Graph traversal failed to expand nodes.\")\n            return [], []\n\n        # 4. Filter Subgraph (Similarity Reranking)\n        filtered_nodes, filtered_rels = self._filter_subgraph_by_similarity(subgraph_nodes, subgraph_rels, query_vector, filter_top_k)\n\n        print(f\"--- Retrieval Pipeline Complete ---\")\n        return filtered_nodes, filtered_rels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming graph_constructor, llm, encoder are initialized correctly\n\n# 1. Create the retriever instance\nretriever = CodeRetriever(graph_constructor, llm, encoder)\n\n# 2. Define your query\nuser_query = \"How do I implement JWT authentication for the login route?\"\n\n# 3. Run the retrieval process\n# Adjust parameters as needed\nretrieved_nodes, retrieved_relationships = retriever.retrieve(\n    user_query,\n    initial_top_k=5,  # How many initial results from hybrid search\n    num_hops=2,       # How far to explore the graph (context depth)\n    filter_top_k=20   # How many description nodes to keep after filtering\n)\n\n# 4. Prepare context for Phase 3 (Code Generation)\ndef format_subgraph_for_llm(nodes: list[dict], rels: list[dict]) -> str:\n    \"\"\"Formats the retrieved subgraph into a string suitable for LLM context.\"\"\"\n    context_str = \"### Retrieved Code Context:\\n\\n\"\n    node_map = {node['id']: node for node in nodes} # Map ID to node data\n\n    # Prioritize Code elements and their definitions/descriptions\n    processed_node_ids = set()\n\n    for node in nodes:\n        if node['id'] in processed_node_ids: continue\n\n        node_labels = node.get('labels', [])\n        node_type = node.get('type', node_labels[0] if node_labels else 'Unknown') # Use 'type' if available\n        node_fqn = node.get('fqn', f\"NodeID_{node['id']}\")\n        node_name = node.get('name', node_fqn)\n\n        # Display Code Nodes (Class, Function, Method)\n        if node_type in ['Class', 'Function', 'Method']:\n            processed_node_ids.add(node['id'])\n            context_str += f\"--- {node_type}: {node_name} ---\\n\"\n            # Find its definition and description\n            def_node = None\n            desc_node = None\n            for rel in rels:\n                 if rel['startNodeId'] == node['id'] and rel['type'] == 'HAS_DEFINITION':\n                      def_node = node_map.get(rel['endNodeId'])\n                      if def_node:\n                           processed_node_ids.add(def_node['id'])\n                           for rel2 in rels:\n                               if rel2['startNodeId'] == def_node['id'] and rel2['type'] == 'HAS_DESCRIPTION':\n                                    desc_node = node_map.get(rel2['endNodeId'])\n                                    if desc_node: processed_node_ids.add(desc_node['id'])\n                                    break\n                      break # Assume only one definition\n\n            if def_node and def_node.get('code'):\n                context_str += f\"```\\n{def_node.get('code')}\\n```\\n\"\n            if def_node and def_node.get('docstring'):\n                context_str += f\"Docstring: {def_node.get('docstring')}\\n\"\n            if desc_node and desc_node.get('description'):\n                context_str += f\"LLM Description: {desc_node.get('description')}\\n\"\n            context_str += \"\\n\"\n\n    # Optionally add relationship information\n    context_str += \"### Key Relationships:\\n\"\n    rels_to_show = ['CALLS', 'IMPORTS', 'HAS_METHOD', 'HAS_ATTRIBUTE'] # Focus on dependencies\n    for rel in rels:\n        if rel['type'] in rels_to_show:\n             start_node = node_map.get(rel['startNodeId'])\n             end_node = node_map.get(rel['endNodeId'])\n             if start_node and end_node:\n                  start_name = start_node.get('name', start_node.get('fqn', f\"ID_{start_node['id']}\"))\n                  end_name = end_node.get('name', end_node.get('fqn', f\"ID_{end_node['id']}\"))\n                  context_str += f\"- {start_name} --[{rel['type']}]--> {end_name}\\n\"\n\n    return context_str\n\n# Generate the context string\ncontext_string_for_llm = format_subgraph_for_llm(retrieved_nodes, retrieved_relationships)\n\nprint(\"\\n=== Formatted Context for LLM (Phase 3) ===\")\nprint(context_string_for_llm[:2000] + \"\\n...\" if len(context_string_for_llm) > 2000 else context_string_for_llm)\n\n# Now, you would combine this context_string_for_llm with the user_query\n# and send it to your code generation LLM (Phase 3).","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"How do I implement JWT authentication for the login route?\"\nprompt = f\"\"\"\n        Analyze the following user query about a software project. Identify any specific function names,\n        class names, method names, variable names, or module names mentioned.\n        Extract only these names. Do not include generic terms like 'function', 'class', 'file', etc.\n        Return the result ONLY as a JSON list of strings. If no specific names are found, return an empty list [].\n\n        User Query: \"{query}\"\n\n        JSON List of Names:\n        \"\"\"\nresponse = llm.invoke(prompt)\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}